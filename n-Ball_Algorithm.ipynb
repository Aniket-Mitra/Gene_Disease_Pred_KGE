{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bda-lab/KGembedding/blob/main/ElEm_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train-Valid Split for n-Ball Model"
      ],
      "metadata": {
        "id": "tEtM3pPSORRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_PATH=\"{}.{}\"\n",
        "FILE_NAME=\"hasAnnotation42.owl\"  #name of finalized ontology (.owl)\n",
        "\n",
        "file_path = FILE_NAME\n",
        "  #print(SPLIT)\n",
        "file = {}\n",
        "  # count = {}\n",
        "for k in ['train', 'valid', 'test']:\n",
        "\n",
        "    #file[k] = open(SAVE_PATH.format(DATASET, SPLIT, k, \"txt\"), 'w')\n",
        "    file[k] = open(SAVE_PATH.format( k, \"txt\"), 'w')\n",
        "    # count[k] = 0\n",
        "file['train_norm'] = open(SAVE_PATH.format( \"train_norm\", \"owl\"), 'w')\n",
        "  # count['train_norm'] = 0\n",
        "subclass_axioms = []\n",
        "other_axioms = []\n",
        "co=0\n",
        "train = 0\n",
        "valid = 0\n",
        "test = 0\n",
        "classes = {}\n",
        "relations = {}\n",
        "subclass_classes = {}\n",
        "  # First pass: Get all classes in subclass axioms, number of axioms\n",
        "with open(file_path) as f:\n",
        "    for line in f:\n",
        "      co+=1\n",
        "      og_line = line\n",
        "      if line.startswith(\"SubClassOf\"):\n",
        "        line = line.strip()[11:-1]\n",
        "        if not line:\n",
        "          co-=1\n",
        "          continue\n",
        "        if not line.startswith(\"ObjectIntersectionOf(\") and not line.startswith(\"ObjectSomeValuesFrom(\") and line.find(\"ObjectSomeValuesFrom(\") == -1:\n",
        "          # SubClassOf C D\n",
        "          it = line.split(' ')\n",
        "          c = it[0]\n",
        "          d = it[1]\n",
        "          # subclass_axioms.append((og_line, c, d))\n",
        "          if c not in subclass_classes:\n",
        "            subclass_classes[c] = len(subclass_classes)\n",
        "          if d not in subclass_classes:\n",
        "            subclass_classes[d] = len(subclass_classes)\n",
        "\n",
        "train = int(co*0.7)\n",
        "valid = int(co*0.3)\n",
        "test = co - train- valid\n",
        "if test>0:\n",
        "     valid+=test\n",
        "     test=0\n",
        "ex_train = train\n",
        "total_train = train\n",
        "count=0\n",
        "print(\"total\", co)\n",
        "print(\"Expected train, valid, test:\", train, valid, test)\n",
        "\n",
        "  # Second pass: add all relation (rbox) axioms and only add non subclass axioms containing classes in subclass axioms\n",
        "with open(file_path) as f:\n",
        "    for line in f:\n",
        "      og_line = line\n",
        "      if line.startswith('SubObjectPropertyOf'):\n",
        "        line = line.strip()[20:-1]\n",
        "        if line.startswith('ObjectPropertyChain'):\n",
        "          line_chain = line.strip()[20:-1]\n",
        "          line1 = line.split(\")\")\n",
        "          line10 = line1[0].split()\n",
        "          if len(line10) < 2:\n",
        "            continue\n",
        "          r1 = line10[0]\n",
        "          r2 = line10[1]\n",
        "          r3 = line1[1]\n",
        "          # if train and (r1 not in relations or r2 not in relations or r3 not in relations):\n",
        "          file['train_norm'].write(og_line)\n",
        "          file['train'].write(r1 + ' ' + r2 + '\\n')\n",
        "          count+=1\n",
        "          train-=1\n",
        "            # if r1 not in relations:\n",
        "            #   relations[r1] = len(relations)\n",
        "            # if r2 not in relations:\n",
        "            #   relations[r2] = len(relations)\n",
        "            # if r3 not in relations:\n",
        "            #   relations[r3] = len(relations)\n",
        "          # else:\n",
        "            # other_axioms.append((og_line, r1, r2, r3))\n",
        "        else:\n",
        "          # print(\"Inside sub obj prop\")\n",
        "          it = line.split(' ')\n",
        "          r1 = it[0]\n",
        "          r2 = it[1]\n",
        "          file['train_norm'].write(og_line)\n",
        "          file['train'].write(r1 + ' ' + r2 + '\\n')\n",
        "          count+=1\n",
        "          train-=1\n",
        "          # other_axioms.append((og_line, r1, r2, \"\"))\n",
        "        continue\n",
        "      line = line.strip()[11:-1]\n",
        "#           print(line)\n",
        "      if not line:\n",
        "        print(og_line)\n",
        "        continue\n",
        "      if line.startswith('ObjectIntersectionOf('):\n",
        "        # C and D SubClassOf E\n",
        "        it = line.split(' ')\n",
        "        c = it[0][21:]\n",
        "        d = it[1][:-1]\n",
        "        e = it[2]\n",
        "        if train and (c in subclass_classes or d in subclass_classes or e in subclass_classes):\n",
        "          file['train_norm'].write(og_line)\n",
        "          file['train'].write(c + ' ' + d + '\\n')\n",
        "          count+=1\n",
        "          train-=1\n",
        "          if c in subclass_classes:\n",
        "            subclass_classes.pop(c)\n",
        "          if d in subclass_classes:\n",
        "            subclass_classes.pop(d)\n",
        "          if e in subclass_classes:\n",
        "            subclass_classes.pop(e)\n",
        "          if c not in classes:\n",
        "            classes[c] = len(classes)\n",
        "          if d not in classes:\n",
        "            classes[d] = len(classes)\n",
        "          if e not in classes:\n",
        "            classes[e] = len(classes)\n",
        "        else:\n",
        "          other_axioms.append((og_line, c, d, e))\n",
        "      elif line.startswith('ObjectSomeValuesFrom('):\n",
        "        # R some C SubClassOf D\n",
        "        it = line.split(' ')\n",
        "        r = it[0][21:]\n",
        "        c = it[1][:-1]\n",
        "        d = it[2]\n",
        "        if train and (c in subclass_classes or d in subclass_classes or r not in relations):\n",
        "          file['train_norm'].write(og_line)\n",
        "          file['train'].write(c + ' ' + d + '\\n')\n",
        "          count+=1\n",
        "          train-=1\n",
        "          if c in subclass_classes:\n",
        "            subclass_classes.pop(c)\n",
        "          if d in subclass_classes:\n",
        "            subclass_classes.pop(d)\n",
        "          if c not in classes:\n",
        "            classes[c] = len(classes)\n",
        "          if d not in classes:\n",
        "            classes[d] = len(classes)\n",
        "          if r not in relations:\n",
        "            relations[r] = len(relations)\n",
        "        else:\n",
        "          other_axioms.append((og_line, r, c, d))\n",
        "      elif line.find('ObjectSomeValuesFrom') != -1:\n",
        "        # C SubClassOf R some D\n",
        "        it = line.split(' ')\n",
        "        c = it[0]\n",
        "        r = it[1][21:]\n",
        "        d = it[2][:-1]\n",
        "        if train and (c in subclass_classes or d in subclass_classes or r not in relations):\n",
        "          file['train_norm'].write(og_line)\n",
        "          file['train'].write(c + ' ' + d + '\\n')\n",
        "          count+=1\n",
        "          train-=1\n",
        "          if c in subclass_classes:\n",
        "            subclass_classes.pop(c)\n",
        "          if d in subclass_classes:\n",
        "            subclass_classes.pop(d)\n",
        "          if c not in classes:\n",
        "            classes[c] = len(classes)\n",
        "          if d not in classes:\n",
        "            classes[d] = len(classes)\n",
        "          if r not in relations:\n",
        "            relations[r] = len(relations)\n",
        "        else:\n",
        "          other_axioms.append((og_line, r, c, d))\n",
        "      else:\n",
        "        # C SubClassOf D\n",
        "        it = line.split(' ')\n",
        "        c = it[0]\n",
        "        d = it[1]\n",
        "        subclass_axioms.append((og_line, c, d))\n",
        "print(\"Added to train from other axioms\", ex_train-train, count)\n",
        "print(count+len(subclass_axioms)+len(other_axioms), len(subclass_axioms), len(other_axioms))\n",
        "print(\"Classes not added\", len([k for k in subclass_classes if k not in classes]), len(subclass_classes))\n",
        "ex_train = train\n",
        "# Second pass\n",
        "temp = []\n",
        "for tup in subclass_axioms:\n",
        "    line, c, d = tup\n",
        "    if train and (c in subclass_classes or d in subclass_classes):\n",
        "#       print(line)\n",
        "      file['train_norm'].write(line)\n",
        "      file['train'].write(c + ' ' + d + '\\n')\n",
        "      count+=1\n",
        "      train-=1\n",
        "      if c in subclass_classes:\n",
        "        subclass_classes.pop(c)\n",
        "      if d in subclass_classes:\n",
        "        subclass_classes.pop(d)\n",
        "      if c not in classes:\n",
        "        classes[c] = len(classes)\n",
        "      if d not in classes:\n",
        "        classes[d] = len(classes)\n",
        "    else:\n",
        "      temp.append((line, c, d))\n",
        "subclass_axioms = temp\n",
        "print(count+len(subclass_axioms)+len(other_axioms), len(subclass_axioms), len(other_axioms))\n",
        "print(\"Added to train from subclass axioms\", ex_train-train)\n",
        "ex_train = train\n",
        "\n",
        "print(\"remaining subclass axioms and other axioms\", len(subclass_axioms), len(other_axioms))\n",
        "print(\"train, valid, test\", train, valid, test)\n",
        "\n",
        "if len(subclass_axioms) < test+valid:\n",
        "    # recalculate test, validation, training sample counts\n",
        "    c = len(subclass_axioms)\n",
        "    #valid = int(c*0.66)\n",
        "    valid = int(c)\n",
        "    #test = c - valid\n",
        "   # if test>0:\n",
        "    # valid+=test\n",
        "     #test=0\n",
        "    train = max(0, min(train, int(c*0.7/0.3) - (total_train-train)))\n",
        "ex_train = train\n",
        "\n",
        "print(\"Train, valid, test left\", train, valid, test)\n",
        "\n",
        "temp = []\n",
        "while test and subclass_axioms != []:\n",
        "    line, c, d = subclass_axioms.pop()\n",
        "    if c not in subclass_classes and d not in subclass_classes:\n",
        "      file['test'].write(c + ' ' + d + '\\n')\n",
        "      test-=1\n",
        "      count+=1\n",
        "    else:\n",
        "      temp.append((line, c, d))\n",
        "while valid and subclass_axioms != []:\n",
        "    line, c, d =  subclass_axioms.pop()\n",
        "    if c not in subclass_classes and d not in subclass_classes:\n",
        "      file['valid'].write(c + ' ' + d + '\\n')\n",
        "      valid-=1\n",
        "      count+=1\n",
        "    else:\n",
        "      temp.append((line, c, d))\n",
        "subclass_axioms.extend(temp)\n",
        "while train and other_axioms != []:\n",
        "    line, r, c, d = other_axioms.pop()\n",
        "#     if c in classes and d in classes:\n",
        "    file['train'].write(c + ' ' + d + '\\n')\n",
        "    file['train_norm'].write(line)\n",
        "    count+=1\n",
        "    train-=1\n",
        "print(\"added to train from other axioms\", ex_train-train)\n",
        "ex_train = train\n",
        "while train and subclass_axioms != []:\n",
        "    line, c, d =  subclass_axioms.pop()\n",
        "#     if c in classes or d in classes:\n",
        "    file['train'].write(c + ' ' + d + '\\n')\n",
        "    file['train_norm'].write(line)\n",
        "    count+=1\n",
        "    train -=1\n",
        "print(\"added to train from subclass axioms\", ex_train-train)\n",
        "for k in ['train', 'valid', 'test']:\n",
        "    file[k].close()\n",
        "print(\"Total:\", count)\n",
        "print(\"left\", len(subclass_axioms), len(other_axioms))\n",
        "print(train, test, valid)"
      ],
      "metadata": {
        "id": "bEz1hwMDOPx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the above code gives two main file- **train.txt**, **valid.txt** among all. The **test.txt** file is also is also generated. It has no axioms/data as we don't need it.\n",
        "\n",
        "The 3 files can be renamed and used in ElEm Algorithm."
      ],
      "metadata": {
        "id": "cb-GPQaGPAyZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eg: for the below code we using **HPO17zt4f_data.zip**.\n",
        "\n",
        "It containes **HPO17zt4f_data** folder which further contains\n",
        "\n",
        "It consists of 1. **hasAnnotation42.owl**\n",
        "\n",
        "2. **HPO_train17zt4f.txt** (train.txt renamed)\n",
        "\n",
        "3. **HPO_valid17zt4f.txt** (valid.txt renamed)\n",
        "\n",
        "3. **HPO_test17zt4f.txt** (test.txt renamed)"
      ],
      "metadata": {
        "id": "wn0CutTm_4Nf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next part, the places which needs to be changed are marked by comment  **##CHANGE**. Please change it as per your use case."
      ],
      "metadata": {
        "id": "5jzUJjYlfiKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-Ball Algorithm"
      ],
      "metadata": {
        "id": "6TeCotfZOgyr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXX4Pf30hRXO",
        "outputId": "9cf3f7e1-aede-4aba-e507-22bc7b7b517b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  HPO17zt4f_data.zip\n",
            "   creating: HPO17zt4f_data/\n",
            "  inflating: HPO17zt4f_data/HPO_test17zt4f.txt  \n",
            "  inflating: HPO17zt4f_data/HPO_valid17zt4f.txt  \n",
            "  inflating: HPO17zt4f_data/HPO_train17zt4f.txt  \n",
            "  inflating: HPO17zt4f_data/readme17zt4f.txt  \n",
            "  inflating: HPO17zt4f_data/hasAnnotation42.owl  \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.mkdir('HPO17zt4f_results')  ##CHANGE\n",
        "os.mkdir('HPO17zt4f_results/EmEL_dir')  ##CHANGE\n",
        "!unzip 'HPO17zt4f_data.zip'  ##CHANGE\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "import click as ck\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import function\n",
        "import re\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "from tensorflow.keras.layers import (\n",
        "    Input,\n",
        ")\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import constraints\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
        "from tensorflow.keras import backend as K\n",
        "from scipy.stats import rankdata\n",
        "from tensorflow.python.keras.utils.data_utils import Sequence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "config = tf.compat.v1.ConfigProto(allow_soft_placement=True)\n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.compat.v1.Session(config=config)\n",
        "\n",
        "\n",
        "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yuYNLEviS5P"
      },
      "outputs": [],
      "source": [
        "def load_valid_data(valid_data_file, classes, relations):\n",
        "    data = []\n",
        "    rel = f'SubClassOf'\n",
        "    with open(valid_data_file, 'r') as f:\n",
        "        for line in f:\n",
        "            it = line.strip().split()\n",
        "            id1 = it[0]\n",
        "            id2 = it[1]\n",
        "            if id1 not in classes or id2 not in classes or rel not in relations:\n",
        "                continue\n",
        "            data.append((classes[id1], relations[rel], classes[id2]))\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0Y8ASHqiVor"
      },
      "outputs": [],
      "source": [
        "def load_cls(train_data_file):\n",
        "    train_subs=list()\n",
        "    counter=0\n",
        "    with open(train_data_file,'r') as f:\n",
        "        for line in f:\n",
        "            counter+=1\n",
        "            it = line.strip().split()\n",
        "            cls1 = it[0]\n",
        "            cls2 = it[1]\n",
        "            train_subs.append(cls1)\n",
        "            train_subs.append(cls2)\n",
        "    train_cls = list(set(train_subs))\n",
        "    return train_cls,counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FhBgk35iYXT",
        "outputId": "c10b6cbc-7ce4-4a1b-f3f3-a346102d0798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data samples: 129432\n",
            "Training data classes: 34387\n"
          ]
        }
      ],
      "source": [
        "total_sub_cls=[]\n",
        "train_file = \"HPO17zt4f_data/HPO_train17zt4f.txt\"  ##CHANGE\n",
        "va_file = \"HPO17zt4f_data/HPO_valid17zt4f.txt\"   ##CHANGE\n",
        "test_file = \"HPO17zt4f_data/HPO_test17zt4f.txt\"   ##CHANGE\n",
        "train_sub_cls,train_samples = load_cls(train_file)\n",
        "valid_sub_cls,valid_samples = load_cls(va_file)\n",
        "test_sub_cls,test_samples = load_cls(test_file)\n",
        "total_sub_cls = train_sub_cls + valid_sub_cls + test_sub_cls\n",
        "\n",
        "all_subcls = list(set(total_sub_cls))\n",
        "#print(len(all_subcls))\n",
        "\n",
        "\n",
        "print(\"Training data samples:\",train_samples)\n",
        "print(\"Training data classes:\",len(train_sub_cls))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzMLhCFUijx7"
      },
      "outputs": [],
      "source": [
        "def load_data(filename):\n",
        "    classes = {}\n",
        "    relations = {}\n",
        "    data = {'nf1': [], 'nf2': [], 'nf3': [], 'nf4': [], 'disjoint': []}\n",
        "    with open(filename) as f:\n",
        "        for line in f:\n",
        "            # Ignore SubObjectPropertyOf\n",
        "            if line.startswith('SubObjectPropertyOf'):\n",
        "                #print(\"------->\",line)\n",
        "                line = line.strip()[20:-1]\n",
        "                if \"ObjectPropertyChain( )\" not in line:\n",
        "                    if line.startswith('ObjectPropertyChain'):\n",
        "                        #print(\"A------->\",line)\n",
        "                        continue\n",
        "            # Ignore SubClassOf()\n",
        "            line = line.strip()[11:-1]\n",
        "            if not line:\n",
        "                continue\n",
        "            if line.startswith('ObjectIntersectionOf(') :\n",
        "              #if 'PATO' not in line:\n",
        "                # C and D SubClassOf E\n",
        "                it = line.split(' ')\n",
        "                c = it[0][21:]\n",
        "                d = it[1][:-1]\n",
        "                e = it[2]\n",
        "                if c not in classes:\n",
        "                    classes[c] = len(classes)\n",
        "                if d not in classes:\n",
        "                    classes[d] = len(classes)\n",
        "                if e not in classes:\n",
        "                    classes[e] = len(classes)\n",
        "                form = 'nf2'\n",
        "                if e == 'owl:Nothing':\n",
        "                    form = 'disjoint'\n",
        "                data[form].append((classes[c], classes[d], classes[e]))\n",
        "\n",
        "            elif line.startswith('ObjectSomeValuesFrom('):\n",
        "                # R some C SubClassOf D\n",
        "                it = line.split(' ')\n",
        "                r = it[0][21:]\n",
        "                c = it[1][:-1]\n",
        "                d = it[2]\n",
        "                if c not in classes:\n",
        "                    classes[c] = len(classes)\n",
        "                if d not in classes:\n",
        "                    classes[d] = len(classes)\n",
        "                if r not in relations:\n",
        "                    relations[r] = len(relations)\n",
        "                data['nf4'].append((relations[r], classes[c], classes[d]))\n",
        "            elif line.find('ObjectSomeValuesFrom') != -1:\n",
        "                # C SubClassOf R some D\n",
        "                it = line.split(' ')\n",
        "                c = it[0]\n",
        "                r = it[1][21:]\n",
        "                d = it[2][:-1]\n",
        "                if c not in classes:\n",
        "                    classes[c] = len(classes)\n",
        "                if d not in classes:\n",
        "                    classes[d] = len(classes)\n",
        "                if r not in relations:\n",
        "                    relations[r] = len(relations)\n",
        "                data['nf3'].append((classes[c], relations[r], classes[d]))\n",
        "            else:\n",
        "                # C SubClassOf D\n",
        "                it = line.split(' ')\n",
        "                c = it[0]\n",
        "                d = it[1]\n",
        "                r = 'SubClassOf'\n",
        "                if r not in relations:\n",
        "                    relations[r] = len(relations)\n",
        "                if c not in classes:\n",
        "                    classes[c] = len(classes)\n",
        "                if d not in classes:\n",
        "                    classes[d] = len(classes)\n",
        "                data['nf1'].append((classes[c],relations[r],classes[d]))\n",
        "\n",
        "    # Check if TOP in classes and insert if it is not there\n",
        "    if 'owl:Thing' not in classes:\n",
        "        classes['owl:Thing'] = len(classes)\n",
        "#changing by adding sub classes of train_data ids to prot_ids\n",
        "    prot_ids = []\n",
        "    class_keys = list(classes.keys())\n",
        "    for val in all_subcls:\n",
        "        if val not in class_keys:\n",
        "            cid = len(classes)\n",
        "            classes[val] = cid\n",
        "            prot_ids.append(cid)\n",
        "        else:\n",
        "            prot_ids.append(classes[val])\n",
        "#     for k, v in classes.items():\n",
        "#         if k in all_subcls:\n",
        "#             prot_ids.append(v)\n",
        "\n",
        "    prot_ids = np.array(prot_ids)\n",
        "    prot_ids2=np.array(list(relations.values()))\n",
        "\n",
        "    # Add corrupted triples nf3\n",
        "    n_classes = len(classes)\n",
        "    data['nf_neg'] = []\n",
        "    for c, r, d in data['nf1']:\n",
        "        x = np.random.choice(prot_ids)\n",
        "        while x == c:\n",
        "            x = np.random.choice(prot_ids)\n",
        "\n",
        "        y = np.random.choice(prot_ids)\n",
        "        while y == d:\n",
        "             y = np.random.choice(prot_ids)\n",
        "        data['nf_neg'].append((c, r,x))\n",
        "        data['nf_neg'].append((y, r, d))\n",
        "\n",
        "    for c, r, d in data['nf3']:\n",
        "        x = np.random.choice(prot_ids)\n",
        "        while x == c:\n",
        "            x = np.random.choice(prot_ids)\n",
        "\n",
        "        y = np.random.choice(prot_ids)\n",
        "        while y == d:\n",
        "             y = np.random.choice(prot_ids)\n",
        "        data['nf_neg'].append((c, r,x))\n",
        "        data['nf_neg'].append((y, r, d))\n",
        "\n",
        "    data['radius'] = []\n",
        "    for val in classes:\n",
        "        data['radius'].append(classes[val])\n",
        "\n",
        "\n",
        "    data['nf1'] = np.array(data['nf1'])\n",
        "    data['nf2'] = np.array(data['nf2'])\n",
        "    data['nf3'] = np.array(data['nf3'])\n",
        "    data['nf4'] = np.array(data['nf4'])\n",
        "    data['disjoint'] = np.array(data['disjoint'])\n",
        "    data['top'] = np.array([classes['owl:Thing'],])\n",
        "    data['nf_neg'] = np.array(data['nf_neg'])\n",
        "    data['radius'] = np.array(data['radius'])\n",
        "\n",
        "\n",
        "    for key, val in data.items():\n",
        "        index = np.arange(len(data[key]))\n",
        "        np.random.seed(seed=100)\n",
        "        np.random.shuffle(index)\n",
        "        data[key] = val[index]\n",
        "\n",
        "    return data, classes, relations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_Rv_GXkkAyb"
      },
      "outputs": [],
      "source": [
        "class ELModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, nb_classes, nb_relations, embedding_size, batch_size, margin, reg_norm=1):\n",
        "        super(ELModel, self).__init__()\n",
        "        self.nb_classes = nb_classes\n",
        "        self.nb_relations = nb_relations\n",
        "        self.margin = margin\n",
        "        self.reg_norm = reg_norm\n",
        "        self.batch_size = batch_size\n",
        "        self.inf = 5.0 # For top radius #100\n",
        "        #initialization of class weights and radius\n",
        "        csample_weights = np.random.uniform(low=-1, high=1, size=(nb_classes, embedding_size))\n",
        "        radius_wts = np.random.uniform(low=0, high=1, size=(nb_classes,))\n",
        "        cls_weights = np.column_stack((csample_weights,radius_wts))\n",
        "        cls_weights = cls_weights / np.linalg.norm(\n",
        "            cls_weights, axis=1).reshape(-1, 1)\n",
        "        rel_weights = np.random.uniform(low=-1, high=1, size=(nb_relations, embedding_size))\n",
        "        rel_weights = rel_weights / np.linalg.norm(\n",
        "            rel_weights, axis=1).reshape(-1, 1)\n",
        "        self.cls_embeddings = tf.keras.layers.Embedding(\n",
        "            nb_classes,\n",
        "            embedding_size + 1,\n",
        "            input_length=1,\n",
        "            weights=[cls_weights,])\n",
        "        self.rel_embeddings = tf.keras.layers.Embedding(\n",
        "            nb_relations,\n",
        "            embedding_size,\n",
        "            input_length=1,\n",
        "            weights=[rel_weights,])\n",
        "\n",
        "\n",
        "    def call(self, input):\n",
        "        \"\"\"Run the model.\"\"\"\n",
        "\n",
        "        nf1, nf2, nf3, nf4,dis,top, nf_neg,radius = input\n",
        "        loss1 = self.nf1_loss(nf1)\n",
        "        loss2 = self.nf2_loss(nf2)\n",
        "        loss3 = self.nf3_loss(nf3)\n",
        "        loss4 = self.nf4_loss(nf4)\n",
        "        loss_dis = self.dis_loss(dis)\n",
        "        loss_top = self.top_loss(top)\n",
        "        loss_nf_neg = self.nf_neg_loss(nf_neg)\n",
        "        loss5 = self.radius_loss(radius)\n",
        "        loss = loss1 +loss3 + loss4 +loss_dis+loss_top+loss_nf_neg - loss5\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def reg(self, x):\n",
        "        res = tf.abs(tf.norm(x, axis=1) - self.reg_norm)\n",
        "        res = tf.reshape(res, [-1, 1])\n",
        "        return res\n",
        "\n",
        "    def nf1_loss(self, input):\n",
        "        c = input[:, 0]\n",
        "        r = input[:,1]\n",
        "        d = input[:, 2]\n",
        "        c = self.cls_embeddings(c)\n",
        "        d = self.cls_embeddings(d)\n",
        "        r = self.rel_embeddings(r)\n",
        "\n",
        "        rc = tf.math.maximum(0.0,c[:, -1])\n",
        "        rd = tf.math.maximum(0.0,d[:, -1])\n",
        "        x1 = c[:, 0:-1]\n",
        "        x2 = d[:, 0:-1]\n",
        "        x3 = x1 + r\n",
        "        euc = tf.norm(x3 - x2, axis=1)\n",
        "        dst = tf.reshape(tf.nn.relu(euc + rc - rd - self.margin), [-1, 1])\n",
        "        return dst + self.reg(x1) + self.reg(x2)\n",
        "\n",
        "    def nf2_loss(self, input):\n",
        "        c = input[:, 0]\n",
        "        d = input[:, 1]\n",
        "        e = input[:, 2]\n",
        "        c = self.cls_embeddings(c)\n",
        "        d = self.cls_embeddings(d)\n",
        "        e = self.cls_embeddings(e)\n",
        "        rc = tf.reshape(tf.math.maximum(0.0,c[:, -1]), [-1, 1])\n",
        "        rd = tf.reshape(tf.math.maximum(0.0,d[:, -1]), [-1, 1])\n",
        "        re = tf.reshape(tf.math.maximum(0.0,e[:, -1]), [-1, 1])\n",
        "        sr = rc + rd\n",
        "        x1 = c[:, 0:-1]\n",
        "        x2 = d[:, 0:-1]\n",
        "        x3 = e[:, 0:-1]\n",
        "\n",
        "        x = x2 - x1\n",
        "        dst = tf.reshape(tf.norm(x, axis=1), [-1, 1])\n",
        "        dst2 = tf.reshape(tf.norm(x3 - x1, axis=1), [-1, 1])\n",
        "        dst3 = tf.reshape(tf.norm(x3 - x2, axis=1), [-1, 1])\n",
        "        rdst = tf.nn.relu(tf.math.minimum(rc, rd) - re - self.margin)\n",
        "\n",
        "        dst_loss=((tf.nn.relu(tf.math.log(dst)-tf.math.log(sr)-self.margin))\n",
        "        +tf.nn.relu(tf.nn.relu(tf.math.log(dst2)-tf.math.log(rc)-self.margin))\n",
        "        +tf.nn.relu(tf.nn.relu(tf.math.log(dst3)-tf.math.log(rd)-self.margin))+rdst)\n",
        "        return dst_loss + self.reg(x1) + self.reg(x2) + self.reg(x3)\n",
        "\n",
        "    def nf3_loss(self, input):\n",
        "        # C subClassOf R some D\n",
        "        c = input[:, 0]\n",
        "        r = input[:, 1]\n",
        "        d = input[:, 2]\n",
        "        c = self.cls_embeddings(c)\n",
        "        d = self.cls_embeddings(d)\n",
        "        r = self.rel_embeddings(r)\n",
        "        x1 = c[:, 0:-1]\n",
        "        x2 = d[:, 0:-1]\n",
        "        x3 = x1 + r\n",
        "\n",
        "        rc = tf.math.maximum(0.0,c[:, -1])\n",
        "        rd = tf.math.maximum(0.0,d[:, -1])\n",
        "        euc = tf.norm(x3 - x2, axis=1)\n",
        "        dst = tf.reshape(tf.nn.relu(euc + rc - rd - self.margin), [-1, 1])\n",
        "\n",
        "        return dst + self.reg(x1) + self.reg(x2)\n",
        "\n",
        "    def nf_neg_loss(self, input):\n",
        "        # C subClassOf R some D\n",
        "        c = input[:, 0]\n",
        "        r = input[:, 1]\n",
        "        d = input[:, 2]\n",
        "        c = self.cls_embeddings(c)\n",
        "        d = self.cls_embeddings(d)\n",
        "        r = self.rel_embeddings(r)\n",
        "        x1 = c[:, 0:-1]\n",
        "        x2 = d[:, 0:-1]\n",
        "\n",
        "        x3 = x1 + r\n",
        "\n",
        "        rc = tf.math.maximum(0.0,c[:, -1])\n",
        "        rd = tf.math.maximum(0.0,d[:, -1])\n",
        "        euc = tf.norm(x3 - x2, axis=1)\n",
        "        dst = tf.reshape((-(euc - rc - rd) + self.margin), [-1, 1])\n",
        "\n",
        "        return dst + self.reg(x1) + self.reg(x2)\n",
        "\n",
        "    def nf4_loss(self, input):\n",
        "        # R some C subClassOf D\n",
        "        r = input[:, 0]\n",
        "        c = input[:, 1]\n",
        "        d = input[:, 2]\n",
        "        c = self.cls_embeddings(c)\n",
        "        d = self.cls_embeddings(d)\n",
        "        r = self.rel_embeddings(r)\n",
        "        rc = tf.reshape(tf.math.maximum(0.0,c[:, -1]), [-1, 1])\n",
        "        rd = tf.reshape(tf.math.maximum(0.0,d[:, -1]), [-1, 1])\n",
        "        sr = rc + rd\n",
        "        x1 = c[:, 0:-1]\n",
        "        x2 = d[:, 0:-1]\n",
        "\n",
        "        # c - r should intersect with d\n",
        "        x3 = x1 - r\n",
        "        dst = tf.reshape(tf.norm(x3 - x2, axis=1), [-1, 1])\n",
        "        dst_loss = tf.nn.relu(dst - sr - self.margin)\n",
        "        return dst_loss + self.reg(x1) + self.reg(x2)\n",
        "\n",
        "\n",
        "    def dis_loss(self, input):\n",
        "        c = input[:, 0]\n",
        "        d = input[:, 1]\n",
        "        c = self.cls_embeddings(c)\n",
        "        d = self.cls_embeddings(d)\n",
        "        rc = tf.reshape(tf.math.maximum(0.0,c[:, -1]), [-1, 1])\n",
        "        rd = tf.reshape(tf.math.maximum(0.0,d[:, -1]), [-1, 1])\n",
        "        sr = rc + rd\n",
        "        x1 = c[:, 0:-1]\n",
        "        x2 = d[:, 0:-1]\n",
        "\n",
        "        dst = tf.reshape(tf.norm(x2 - x1, axis=1), [-1, 1])\n",
        "        return tf.nn.relu(sr - dst + self.margin) + self.reg(x1) + self.reg(x2)\n",
        "\n",
        "\n",
        "    def top_loss(self, input):\n",
        "        d = input[:, 0]\n",
        "        d = self.cls_embeddings(d)\n",
        "        rd = tf.reshape(tf.math.maximum(0.0,d[:, -1]), [-1, 1])\n",
        "        return tf.math.abs(rd - self.inf)\n",
        "\n",
        "    def inclusion_loss(self,input):\n",
        "        r1 = input[:, 0]\n",
        "        r2 = input[:, 1]\n",
        "        r1 = self.rel_embeddings(r1)\n",
        "        r2 = self.rel_embeddings(r2)\n",
        "        #print(\"r2 type------>\",type(r2))\n",
        "\n",
        "        euc = tf.norm(r2 - r1, axis=1)\n",
        "\n",
        "        normalize_a = tf.nn.l2_normalize(r1,0)\n",
        "        normalize_b = tf.nn.l2_normalize(r2,0)\n",
        "        direction=tf.reduce_sum(tf.multiply(normalize_a,normalize_b))\n",
        "        dir_loss = tf.abs(1 - direction)\n",
        "        dir_loss = tf.reshape(dir_loss, [-1, 1])\n",
        "        dst = tf.reshape(tf.nn.relu(euc - self.margin), [-1, 1])\n",
        "\n",
        "        return dst + self.reg(r1) + self.reg(r2) + dir_loss\n",
        "\n",
        "    def chain_loss(self,input):\n",
        "        r1 = input[:, 0]\n",
        "        r2 = input[:, 1]\n",
        "        r3 = input[:, 2]\n",
        "        c = self.rel_embeddings(r1)\n",
        "        d = self.rel_embeddings(r2)\n",
        "        e = self.rel_embeddings(r3)\n",
        "        s = e - c\n",
        "\n",
        "        dst = tf.reshape(tf.norm(c - d, axis=1), [-1, 1])\n",
        "        dst2 = tf.reshape(tf.norm(e - c, axis=1), [-1, 1])\n",
        "        dst3 = tf.reshape(tf.norm(e - d, axis=1), [-1, 1])\n",
        "        dst_dir = tf.reshape(tf.norm(s - d, axis=1), [-1, 1])\n",
        "        dst_loss = (tf.nn.relu(dst_dir - self.margin))\n",
        "        res_vec = c + d\n",
        "        normalize_a = tf.nn.l2_normalize(res_vec,0)\n",
        "        normalize_b = tf.nn.l2_normalize(e,0)\n",
        "        direction=tf.reduce_sum(tf.multiply(normalize_a,normalize_b))\n",
        "        dir_loss = tf.abs(1 - direction)\n",
        "        dir_loss = tf.reshape(dir_loss, [-1, 1])\n",
        "        return dst_loss + self.reg(c) + self.reg(d) + self.reg(e) + dir_loss\n",
        "\n",
        "    def radius_loss(self, input):\n",
        "        d = input[:, 0]\n",
        "        d = self.cls_embeddings(d)\n",
        "        rd = tf.reshape(d[:, -1], [-1, 1])\n",
        "        return tf.math.minimum(0.0,rd)\n",
        "\n",
        "\n",
        "# In[10]:\n",
        "\n",
        "\n",
        "class Generator(object):\n",
        "\n",
        "    def __init__(self, data, batch_size, steps=80):  #100\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "        self.steps = steps\n",
        "        self.start = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        return self.next()\n",
        "\n",
        "    def reset(self):\n",
        "        self.start = 0\n",
        "\n",
        "    def next(self):\n",
        "        if self.start < self.steps:\n",
        "            nf1_index = np.random.choice(\n",
        "                self.data['nf1'].shape[0], self.batch_size)\n",
        "            nf2_index = np.random.choice(\n",
        "                self.data['nf2'].shape[0], self.batch_size)\n",
        "            nf3_index = np.random.choice(\n",
        "                self.data['nf3'].shape[0], self.batch_size)\n",
        "            nf4_index = np.random.choice(\n",
        "                self.data['nf4'].shape[0], self.batch_size)\n",
        "            dis_index = np.random.choice(\n",
        "                self.data['disjoint'].shape[0], self.batch_size)\n",
        "            top_index = np.random.choice(\n",
        "                self.data['top'].shape[0], self.batch_size)\n",
        "            nf_neg_index = np.random.choice(\n",
        "                self.data['nf_neg'].shape[0], self.batch_size)\n",
        "            radius_index = np.random.choice(\n",
        "                self.data['radius'].shape[0], self.batch_size)\n",
        "            nf1 = self.data['nf1'][nf1_index]\n",
        "            nf2 = self.data['nf2'][nf2_index]\n",
        "            nf3 = self.data['nf3'][nf3_index]\n",
        "            nf4 = self.data['nf4'][nf4_index]\n",
        "            dis = self.data['disjoint'][dis_index]\n",
        "            top = self.data['top'][top_index]\n",
        "            nf_neg = self.data['nf_neg'][nf_neg_index]\n",
        "            radius = self.data['radius'][radius_index]\n",
        "            labels = np.zeros((self.batch_size, 1), dtype=np.float32)\n",
        "            self.start += 1\n",
        "            return ([nf1, nf2, nf3, nf4,dis,top,nf_neg,radius], labels)\n",
        "        else:\n",
        "            self.reset()\n",
        "\n",
        "\n",
        "# In[11]:\n",
        "\n",
        "\n",
        "class MyModelCheckpoint(ModelCheckpoint):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(ModelCheckpoint, self).__init__()\n",
        "        self.out_classes_file = kwargs.pop('out_classes_file')\n",
        "        self.out_relations_file = kwargs.pop('out_relations_file')\n",
        "        self.monitor = kwargs.pop('monitor')\n",
        "        self.cls_list = kwargs.pop('cls_list')\n",
        "        self.rel_list = kwargs.pop('rel_list')\n",
        "        self.load_weights_on_restart = False\n",
        "        ##\n",
        "        self.save_freq='epoch'\n",
        "        ##\n",
        "        self.valid_data = kwargs.pop('valid_data')\n",
        "        self.proteins = kwargs.pop('proteins')\n",
        "        self.prot_index = list(self.proteins.values())\n",
        "        self.prot_dict = {v: k for k, v in enumerate(self.prot_index)}\n",
        "\n",
        "        self.best_rank = 100000\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Save embeddings every 10 epochs\n",
        "        current_loss = logs.get(self.monitor)\n",
        "        if math.isnan(current_loss):\n",
        "            print('NAN loss, stopping training')\n",
        "            self.model.stop_training = True\n",
        "            return\n",
        "        el_model = self.model.layers[-1]\n",
        "        cls_embeddings = el_model.cls_embeddings.get_weights()[0]\n",
        "        rel_embeddings = el_model.rel_embeddings.get_weights()[0]\n",
        "\n",
        "        prot_embeds = cls_embeddings[self.prot_index]\n",
        "        prot_rs = prot_embeds[:, -1].reshape(-1, 1)\n",
        "        prot_embeds = prot_embeds[:, :-1]\n",
        "        mean_rank = 0\n",
        "        n = len(self.valid_data)\n",
        "\n",
        "        for c, r, d in self.valid_data:\n",
        "            c, r, d = self.prot_dict[c], r, self.prot_dict[d]\n",
        "            ec = prot_embeds[c, :]\n",
        "            rc = prot_rs[c, :]\n",
        "            er = rel_embeddings[r, :]\n",
        "            ec += er\n",
        "\n",
        "            dst = np.linalg.norm(prot_embeds - ec.reshape(1, -1), axis=1)\n",
        "            dst = dst.reshape(-1, 1)\n",
        "            res = np.maximum(0, dst - rc - prot_rs - el_model.margin)\n",
        "            res = res.flatten()\n",
        "            index = rankdata(res, method='average')\n",
        "            rank = index[d]\n",
        "            mean_rank += rank\n",
        "\n",
        "        mean_rank /= n\n",
        "        # fmean_rank /= n\n",
        "        print(f'\\n Validation {epoch + 1} {mean_rank}\\n')\n",
        "\n",
        "        if mean_rank < self.best_rank:\n",
        "            self.best_rank = mean_rank\n",
        "            print(f'\\n Saving embeddings {epoch + 1} {mean_rank}\\n')\n",
        "            cls_file = self.out_classes_file\n",
        "            rel_file = self.out_relations_file\n",
        "\n",
        "\n",
        "            df = pd.DataFrame(\n",
        "                {'classes': self.cls_list, 'embeddings': list(cls_embeddings)})\n",
        "            #print(df.head(1))\n",
        "            df.to_pickle(cls_file)\n",
        "\n",
        "\n",
        "            df = pd.DataFrame(\n",
        "                {'relations': self.rel_list, 'embeddings': list(rel_embeddings)})\n",
        "            df.to_pickle(rel_file)\n",
        "        return epoch\n",
        "\n",
        "\n",
        "\n",
        "def build_model(device,train_data,classes,relations,valid_data):\n",
        "    proteins = {}#substitute for classes with subclass case\n",
        "    for val in all_subcls:\n",
        "        proteins[val] = classes[val]\n",
        "    nb_classes = len(classes)\n",
        "    nb_relations = len(relations)\n",
        "    print(\"no. classes:\",nb_classes)\n",
        "    print(\"no. relations:\",nb_relations)\n",
        "    nb_train_data = 0\n",
        "    for key, val in train_data.items():\n",
        "        nb_train_data = max(len(val), nb_train_data)\n",
        "    train_steps = int(math.ceil(nb_train_data / (1.0 * batch_size)))\n",
        "    train_generator = Generator(train_data, batch_size, steps=train_steps)\n",
        "\n",
        "    cls_dict = {v: k for k, v in classes.items()}\n",
        "    rel_dict = {v: k for k, v in relations.items()}\n",
        "\n",
        "    cls_list = []\n",
        "    rel_list = []\n",
        "    for i in range(nb_classes):\n",
        "        cls_list.append(cls_dict[i])\n",
        "    for i in range(nb_relations):\n",
        "        rel_list.append(rel_dict[i])\n",
        "\n",
        "    with tf.device('/' + device):\n",
        "        nf1 = Input(shape=(3,), dtype=np.int32)\n",
        "        nf2 = Input(shape=(3,), dtype=np.int32)\n",
        "        nf3 = Input(shape=(3,), dtype=np.int32)\n",
        "        nf4 = Input(shape=(3,), dtype=np.int32)\n",
        "        dis = Input(shape=(3,), dtype=np.int32)\n",
        "        top = Input(shape=(1,), dtype=np.int32)\n",
        "        nf_neg = Input(shape=(3,), dtype=np.int32)\n",
        "        radius = Input(shape=(1,), dtype=np.int32)\n",
        "        el_model = ELModel(nb_classes, nb_relations, embedding_size, batch_size, margin, reg_norm)\n",
        "        out = el_model(input=[nf1,nf2, nf3, nf4,dis,top, nf_neg,radius])\n",
        "        model = tf.keras.Model(inputs=[nf1,nf2, nf3, nf4, dis,top,nf_neg,radius], outputs=out)\n",
        "\n",
        "        optimizer = optimizers.Adam(lr=learning_rate)\n",
        "\n",
        "\n",
        "        model.compile(optimizer=optimizer, loss=\"mse\")\n",
        "\n",
        "    # TOP Embedding\n",
        "        top = classes.get('owl:Thing', None)\n",
        "        checkpointer = MyModelCheckpoint(\n",
        "            out_classes_file=out_classes_file,\n",
        "            out_relations_file=out_relations_file,\n",
        "            cls_list=cls_list,\n",
        "            rel_list=rel_list,\n",
        "            valid_data=valid_data[0:100],\n",
        "            proteins=proteins,\n",
        "            monitor='loss')\n",
        "\n",
        "\n",
        "        logger = CSVLogger(loss_history_file)\n",
        "\n",
        "        # Save initial embeddings\n",
        "        cls_embeddings = el_model.cls_embeddings.get_weights()[0]\n",
        "        rel_embeddings = el_model.rel_embeddings.get_weights()[0]\n",
        "\n",
        "        cls_file = out_classes_file\n",
        "        rel_file = out_relations_file\n",
        "\n",
        "\n",
        "        df = pd.DataFrame(\n",
        "            {'classes': cls_list, 'embeddings': list(cls_embeddings)})\n",
        "        df.to_pickle(cls_file)\n",
        "        df = pd.DataFrame(\n",
        "            {'relations': rel_list, 'embeddings': list(rel_embeddings)})\n",
        "        df.to_pickle(rel_file)\n",
        "\n",
        "        model.fit_generator(\n",
        "            train_generator,\n",
        "            steps_per_epoch=train_steps,\n",
        "            epochs=epochs,\n",
        "            workers=12,\n",
        "            callbacks=[logger, checkpointer])\n",
        "\n",
        "\n",
        "# In[13]:\n",
        "\n",
        "\n",
        "\n",
        "gdata_file=\"HPO17zt4f_data/hasAnnotation42.owl\"  ##CHANGE\n",
        "train_data_model, classes, relations = load_data(gdata_file)\n",
        "\n",
        "\n",
        "# In[14]:\n",
        "\n",
        "\n",
        "valid_data_file=\"HPO17zt4f_data/HPO_valid17zt4f.txt\"  ##CHANGE\n",
        "valid_data_model = load_valid_data(valid_data_file, classes, relations)\n",
        "\n",
        "\n",
        "\n",
        "margins = [-0.1]  ##CHANGE\n",
        "embed_dims = [100]  ##CHANGE\n",
        "batch_size =   256\n",
        "device='gpu:0'\n",
        "reg_norm=1\n",
        "learning_rate= 3e-4\n",
        "epochs=1000\n",
        "for d in embed_dims:\n",
        "    embedding_size = d\n",
        "    print(\"***************Embedding Dim:\",embedding_size,'****************')\n",
        "    for m in margins:\n",
        "        margin = m\n",
        "        print(\"**************Margin Loss:\",margin,\"***************\")\n",
        "        out_classes_file = f'HPO17zt4f_results/EmEL_dir/HPO17zt4f_{embedding_size}_{margin}_{epochs}_cls.pkl'  ##CHANGE\n",
        "        out_relations_file = f'HPO17zt4f_results/EmEL_dir/HPO17zt4f_{embedding_size}_{margin}_{epochs}_rel.pkl'  ##CHANGE\n",
        "        loss_history_file= f'HPO17zt4f_results/EmEL_dir/HPO17zt4f_lossHis_{embedding_size}_{margin}_{epochs}.csv'  ##CHANGE\n",
        "        build_model(device,train_data_model,classes,relations,valid_data_model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
